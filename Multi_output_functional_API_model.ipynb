{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi output functional API model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_PgySJunoX-"
      },
      "source": [
        "Using Energy efficiency dataset to train the model.\\\n",
        "The dataset is available on UCI Machine Learning Repository.\\\n",
        "\n",
        "Link of dataset: https://drive.google.com/file/d/1H5DBqCnl_fXNCO6KScJEH8iCKsRc0epD/view?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHbLjzo4jxjG"
      },
      "source": [
        "Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "on8V9LDiYXhu"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.layers import Dense, Input\n",
        "from keras.models import Sequential"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8v42sTf3hyQn",
        "outputId": "1a3c87cb-ed09-45c5-c5aa-d098855f6e68"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lLuc6maLGOd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "d3ca9e64-d5c2-452e-9d55-155c6c48cc50"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/ENB2012_data.csv')\n",
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X1</th>\n",
              "      <th>X2</th>\n",
              "      <th>X3</th>\n",
              "      <th>X4</th>\n",
              "      <th>X5</th>\n",
              "      <th>X6</th>\n",
              "      <th>X7</th>\n",
              "      <th>X8</th>\n",
              "      <th>Y1</th>\n",
              "      <th>Y2</th>\n",
              "      <th>Unnamed: 10</th>\n",
              "      <th>Unnamed: 11</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.98</td>\n",
              "      <td>514.5</td>\n",
              "      <td>294.0</td>\n",
              "      <td>110.25</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.55</td>\n",
              "      <td>21.33</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.98</td>\n",
              "      <td>514.5</td>\n",
              "      <td>294.0</td>\n",
              "      <td>110.25</td>\n",
              "      <td>7.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.55</td>\n",
              "      <td>21.33</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.98</td>\n",
              "      <td>514.5</td>\n",
              "      <td>294.0</td>\n",
              "      <td>110.25</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.55</td>\n",
              "      <td>21.33</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.98</td>\n",
              "      <td>514.5</td>\n",
              "      <td>294.0</td>\n",
              "      <td>110.25</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.55</td>\n",
              "      <td>21.33</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.90</td>\n",
              "      <td>563.5</td>\n",
              "      <td>318.5</td>\n",
              "      <td>122.50</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.84</td>\n",
              "      <td>28.28</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     X1     X2     X3      X4   X5  ...   X8     Y1     Y2  Unnamed: 10  Unnamed: 11\n",
              "0  0.98  514.5  294.0  110.25  7.0  ...  0.0  15.55  21.33          NaN          NaN\n",
              "1  0.98  514.5  294.0  110.25  7.0  ...  0.0  15.55  21.33          NaN          NaN\n",
              "2  0.98  514.5  294.0  110.25  7.0  ...  0.0  15.55  21.33          NaN          NaN\n",
              "3  0.98  514.5  294.0  110.25  7.0  ...  0.0  15.55  21.33          NaN          NaN\n",
              "4  0.90  563.5  318.5  122.50  7.0  ...  0.0  20.84  28.28          NaN          NaN\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBLmOmvi8_4Z",
        "outputId": "19c8dc13-b717-4156-92db-09b9673db632"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1296, 12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFw1hahJ83f4",
        "outputId": "2e78a37b-2d85-46e4-bfdb-ceb902a5d28b"
      },
      "source": [
        "df.isna().sum()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "X1              528\n",
              "X2              528\n",
              "X3              528\n",
              "X4              528\n",
              "X5              528\n",
              "X6              528\n",
              "X7              528\n",
              "X8              528\n",
              "Y1              528\n",
              "Y2              528\n",
              "Unnamed: 10    1296\n",
              "Unnamed: 11    1296\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_Dpk6fbkQKw"
      },
      "source": [
        "The dataset has 8 independent features and 2 dependent features.\\\n",
        "Removing the unwanted columns and rows with NULL entries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fECSjI4f9Jp5",
        "outputId": "0ad40c5e-1d94-4fa5-beeb-c5954ed48594"
      },
      "source": [
        "df.drop(['Unnamed: 10','Unnamed: 11'],axis='columns',inplace=True)\n",
        "df.dropna(axis = 'rows', inplace=True)\n",
        "df.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(768, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "A9313WlNLe3D",
        "outputId": "1c7adc2c-2592-46a1-90c4-b8e99715176d"
      },
      "source": [
        "x = df.iloc[:,:8]\n",
        "y1 = df['Y1']\n",
        "y2 = df['Y2']\n",
        "x.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X1</th>\n",
              "      <th>X2</th>\n",
              "      <th>X3</th>\n",
              "      <th>X4</th>\n",
              "      <th>X5</th>\n",
              "      <th>X6</th>\n",
              "      <th>X7</th>\n",
              "      <th>X8</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.98</td>\n",
              "      <td>514.5</td>\n",
              "      <td>294.0</td>\n",
              "      <td>110.25</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.98</td>\n",
              "      <td>514.5</td>\n",
              "      <td>294.0</td>\n",
              "      <td>110.25</td>\n",
              "      <td>7.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.98</td>\n",
              "      <td>514.5</td>\n",
              "      <td>294.0</td>\n",
              "      <td>110.25</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.98</td>\n",
              "      <td>514.5</td>\n",
              "      <td>294.0</td>\n",
              "      <td>110.25</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.90</td>\n",
              "      <td>563.5</td>\n",
              "      <td>318.5</td>\n",
              "      <td>122.50</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     X1     X2     X3      X4   X5   X6   X7   X8\n",
              "0  0.98  514.5  294.0  110.25  7.0  2.0  0.0  0.0\n",
              "1  0.98  514.5  294.0  110.25  7.0  3.0  0.0  0.0\n",
              "2  0.98  514.5  294.0  110.25  7.0  4.0  0.0  0.0\n",
              "3  0.98  514.5  294.0  110.25  7.0  5.0  0.0  0.0\n",
              "4  0.90  563.5  318.5  122.50  7.0  2.0  0.0  0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcC0fkuwOqHC",
        "outputId": "a592fa0b-5197-425c-b37e-01c959e4f19d"
      },
      "source": [
        "y1 = np.array(y1)\n",
        "y2 = np.array(y2)\n",
        "y1[:5],y2[:5]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([15.55, 15.55, 15.55, 15.55, 20.84]),\n",
              " array([21.33, 21.33, 21.33, 21.33, 28.28]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vri0bpIYPwTK",
        "outputId": "98fdb898-92bb-4dae-e5cb-87299c7fb529"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "std = StandardScaler()\n",
        "std.fit(x)\n",
        "x = std.transform(x)\n",
        "print(x[0])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2.04177671 -1.78587489 -0.56195149 -1.47007664  1.         -1.34164079\n",
            " -1.76044698 -1.81457514]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSSJNprfRK9a"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y1_train,y1_test,y2_train,y2_test = train_test_split(x,y1,y2,test_size=0.2)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0Oj8NoqRfPy",
        "outputId": "d522d3cd-6f83-4a5b-c7d1-f8ca4bb6b9ed"
      },
      "source": [
        "y1_train.shape,y2_train.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((614,), (614,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJ6DBYdOB-jf"
      },
      "source": [
        "Models training and prediction using Functional API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGXuzfqb9iTF"
      },
      "source": [
        "from keras.models import Model\n",
        "\n",
        "def build_model():\n",
        "  input = Input(shape=(8,))\n",
        "  dense1 = Dense(128,activation='relu')(input)\n",
        "  dense2 = Dense(64, activation = 'relu')(dense1)\n",
        "\n",
        "  output1 = Dense(1, name='out1')(dense2)\n",
        "  dense3 = Dense(32,activation = 'relu')(dense2)\n",
        "  output2 = Dense(1, name = 'out2')(dense3)\n",
        "\n",
        "  model = Model(inputs=input,outputs=[output1,output2])\n",
        "  return model"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E95_6osm9iWX"
      },
      "source": [
        "model = build_model()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQnQxY0v9iYK"
      },
      "source": [
        "model.compile(optimizer='adam',loss = {'out1':'mse','out2':'mse'})"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8-BQwq39imE",
        "outputId": "8c7db4ee-e9d3-4ad8-a7e1-d70a878fa905"
      },
      "source": [
        "his = model.fit(x = x_train, y=[y1_train,y2_train],epochs=100)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "20/20 [==============================] - 13s 2ms/step - loss: 1208.7534 - out1_loss: 562.0540 - out2_loss: 646.6993\n",
            "Epoch 2/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1024.1001 - out1_loss: 483.9146 - out2_loss: 540.1855\n",
            "Epoch 3/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 524.3181 - out1_loss: 317.2286 - out2_loss: 207.0895\n",
            "Epoch 4/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 203.1815 - out1_loss: 140.7098 - out2_loss: 62.4717\n",
            "Epoch 5/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 93.4807 - out1_loss: 55.0778 - out2_loss: 38.4029\n",
            "Epoch 6/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 65.0033 - out1_loss: 32.2871 - out2_loss: 32.7162\n",
            "Epoch 7/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 53.6398 - out1_loss: 25.4440 - out2_loss: 28.1957\n",
            "Epoch 8/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 47.2804 - out1_loss: 23.0373 - out2_loss: 24.2432\n",
            "Epoch 9/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 45.4121 - out1_loss: 21.9992 - out2_loss: 23.4129\n",
            "Epoch 10/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 42.5911 - out1_loss: 19.5516 - out2_loss: 23.0395\n",
            "Epoch 11/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 38.1256 - out1_loss: 18.1049 - out2_loss: 20.0207\n",
            "Epoch 12/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 33.1600 - out1_loss: 15.6949 - out2_loss: 17.4650\n",
            "Epoch 13/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 35.7993 - out1_loss: 17.4332 - out2_loss: 18.3660\n",
            "Epoch 14/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 33.1599 - out1_loss: 15.2448 - out2_loss: 17.9152\n",
            "Epoch 15/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 25.0735 - out1_loss: 12.0223 - out2_loss: 13.0511\n",
            "Epoch 16/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 27.9243 - out1_loss: 14.3223 - out2_loss: 13.6020\n",
            "Epoch 17/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 25.2027 - out1_loss: 12.6454 - out2_loss: 12.5573\n",
            "Epoch 18/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 22.2520 - out1_loss: 11.0052 - out2_loss: 11.2467\n",
            "Epoch 19/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 21.5679 - out1_loss: 10.3454 - out2_loss: 11.2225\n",
            "Epoch 20/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 22.7378 - out1_loss: 11.5341 - out2_loss: 11.2037\n",
            "Epoch 21/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 18.9077 - out1_loss: 8.9128 - out2_loss: 9.9949\n",
            "Epoch 22/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 19.5236 - out1_loss: 9.2493 - out2_loss: 10.2743\n",
            "Epoch 23/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 19.5064 - out1_loss: 9.0976 - out2_loss: 10.4088\n",
            "Epoch 24/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 18.0714 - out1_loss: 8.0795 - out2_loss: 9.9919\n",
            "Epoch 25/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 17.9606 - out1_loss: 8.8175 - out2_loss: 9.1432\n",
            "Epoch 26/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 17.9968 - out1_loss: 8.2676 - out2_loss: 9.7293\n",
            "Epoch 27/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 19.5313 - out1_loss: 9.0538 - out2_loss: 10.4774\n",
            "Epoch 28/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 17.7062 - out1_loss: 8.6056 - out2_loss: 9.1006\n",
            "Epoch 29/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 19.0021 - out1_loss: 8.9185 - out2_loss: 10.0836\n",
            "Epoch 30/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 17.5996 - out1_loss: 8.4016 - out2_loss: 9.1980\n",
            "Epoch 31/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 16.9696 - out1_loss: 8.1473 - out2_loss: 8.8223\n",
            "Epoch 32/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 16.6282 - out1_loss: 7.6185 - out2_loss: 9.0097\n",
            "Epoch 33/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 16.6510 - out1_loss: 8.2032 - out2_loss: 8.4478\n",
            "Epoch 34/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 16.5887 - out1_loss: 7.5696 - out2_loss: 9.0192\n",
            "Epoch 35/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 16.5967 - out1_loss: 7.5652 - out2_loss: 9.0315\n",
            "Epoch 36/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 16.5847 - out1_loss: 8.1584 - out2_loss: 8.4263\n",
            "Epoch 37/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 16.9271 - out1_loss: 8.0113 - out2_loss: 8.9158\n",
            "Epoch 38/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 15.7716 - out1_loss: 7.6063 - out2_loss: 8.1653\n",
            "Epoch 39/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 15.8712 - out1_loss: 7.2876 - out2_loss: 8.5836\n",
            "Epoch 40/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 15.5652 - out1_loss: 7.5511 - out2_loss: 8.0142\n",
            "Epoch 41/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 14.4095 - out1_loss: 6.8163 - out2_loss: 7.5932\n",
            "Epoch 42/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 15.3222 - out1_loss: 7.5463 - out2_loss: 7.7759\n",
            "Epoch 43/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 16.5856 - out1_loss: 8.1621 - out2_loss: 8.4236\n",
            "Epoch 44/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 15.9881 - out1_loss: 7.5481 - out2_loss: 8.4401\n",
            "Epoch 45/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 14.1320 - out1_loss: 6.8368 - out2_loss: 7.2952\n",
            "Epoch 46/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 14.4188 - out1_loss: 7.3142 - out2_loss: 7.1046\n",
            "Epoch 47/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 16.9129 - out1_loss: 7.3785 - out2_loss: 9.5345\n",
            "Epoch 48/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 13.5730 - out1_loss: 6.6952 - out2_loss: 6.8778\n",
            "Epoch 49/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 15.9080 - out1_loss: 7.4579 - out2_loss: 8.4500\n",
            "Epoch 50/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 13.6466 - out1_loss: 6.6076 - out2_loss: 7.0390\n",
            "Epoch 51/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 13.7487 - out1_loss: 6.6371 - out2_loss: 7.1116\n",
            "Epoch 52/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 15.0051 - out1_loss: 7.1511 - out2_loss: 7.8540\n",
            "Epoch 53/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 14.8101 - out1_loss: 6.7763 - out2_loss: 8.0338\n",
            "Epoch 54/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 14.5529 - out1_loss: 7.1244 - out2_loss: 7.4285\n",
            "Epoch 55/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 15.0292 - out1_loss: 7.0227 - out2_loss: 8.0065\n",
            "Epoch 56/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 14.0259 - out1_loss: 6.5303 - out2_loss: 7.4956\n",
            "Epoch 57/100\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 12.3899 - out1_loss: 5.8325 - out2_loss: 6.5575\n",
            "Epoch 58/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 14.6374 - out1_loss: 6.8238 - out2_loss: 7.8137\n",
            "Epoch 59/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 12.7233 - out1_loss: 5.6924 - out2_loss: 7.0309\n",
            "Epoch 60/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 14.4374 - out1_loss: 6.4920 - out2_loss: 7.9454\n",
            "Epoch 61/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 14.6322 - out1_loss: 6.9308 - out2_loss: 7.7013\n",
            "Epoch 62/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 14.2482 - out1_loss: 6.3760 - out2_loss: 7.8722\n",
            "Epoch 63/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 12.9087 - out1_loss: 6.0525 - out2_loss: 6.8562\n",
            "Epoch 64/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 13.6859 - out1_loss: 6.2583 - out2_loss: 7.4277\n",
            "Epoch 65/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 13.8283 - out1_loss: 6.4471 - out2_loss: 7.3812\n",
            "Epoch 66/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 14.3099 - out1_loss: 6.6617 - out2_loss: 7.6482\n",
            "Epoch 67/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 12.8061 - out1_loss: 6.2662 - out2_loss: 6.5400\n",
            "Epoch 68/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 13.1591 - out1_loss: 6.0120 - out2_loss: 7.1471\n",
            "Epoch 69/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 12.8410 - out1_loss: 6.0011 - out2_loss: 6.8399\n",
            "Epoch 70/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 12.3765 - out1_loss: 5.4818 - out2_loss: 6.8947\n",
            "Epoch 71/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 13.4856 - out1_loss: 6.3890 - out2_loss: 7.0966\n",
            "Epoch 72/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 14.1438 - out1_loss: 6.0285 - out2_loss: 8.1153\n",
            "Epoch 73/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 12.9881 - out1_loss: 5.9847 - out2_loss: 7.0034\n",
            "Epoch 74/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 11.6461 - out1_loss: 5.3537 - out2_loss: 6.2924\n",
            "Epoch 75/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 12.3418 - out1_loss: 5.6523 - out2_loss: 6.6895\n",
            "Epoch 76/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 11.6724 - out1_loss: 5.3765 - out2_loss: 6.2958\n",
            "Epoch 77/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 11.9248 - out1_loss: 5.7544 - out2_loss: 6.1705\n",
            "Epoch 78/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 12.9338 - out1_loss: 5.9696 - out2_loss: 6.9641\n",
            "Epoch 79/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 12.0970 - out1_loss: 5.6646 - out2_loss: 6.4323\n",
            "Epoch 80/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 11.3479 - out1_loss: 5.3912 - out2_loss: 5.9568\n",
            "Epoch 81/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 12.1727 - out1_loss: 5.4381 - out2_loss: 6.7346\n",
            "Epoch 82/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 11.4605 - out1_loss: 5.6466 - out2_loss: 5.8138\n",
            "Epoch 83/100\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 11.9925 - out1_loss: 5.6109 - out2_loss: 6.3816\n",
            "Epoch 84/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 10.5057 - out1_loss: 4.8952 - out2_loss: 5.6105\n",
            "Epoch 85/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 10.9389 - out1_loss: 4.7709 - out2_loss: 6.1680\n",
            "Epoch 86/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 10.1196 - out1_loss: 4.7173 - out2_loss: 5.4023\n",
            "Epoch 87/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 11.1568 - out1_loss: 5.5425 - out2_loss: 5.6142\n",
            "Epoch 88/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 10.5698 - out1_loss: 4.9250 - out2_loss: 5.6448\n",
            "Epoch 89/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 11.2075 - out1_loss: 5.1382 - out2_loss: 6.0693\n",
            "Epoch 90/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 11.1309 - out1_loss: 5.0691 - out2_loss: 6.0619\n",
            "Epoch 91/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 8.6860 - out1_loss: 3.9296 - out2_loss: 4.7564\n",
            "Epoch 92/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 10.0793 - out1_loss: 4.7462 - out2_loss: 5.3331\n",
            "Epoch 93/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 9.6412 - out1_loss: 4.6772 - out2_loss: 4.9640\n",
            "Epoch 94/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 9.3073 - out1_loss: 4.3236 - out2_loss: 4.9837\n",
            "Epoch 95/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 9.4099 - out1_loss: 4.3302 - out2_loss: 5.0797\n",
            "Epoch 96/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 8.8901 - out1_loss: 4.2899 - out2_loss: 4.6002\n",
            "Epoch 97/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 8.0790 - out1_loss: 4.0065 - out2_loss: 4.0725\n",
            "Epoch 98/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 8.9184 - out1_loss: 4.1275 - out2_loss: 4.7910\n",
            "Epoch 99/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 8.0481 - out1_loss: 3.5353 - out2_loss: 4.5128\n",
            "Epoch 100/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 7.8342 - out1_loss: 3.9194 - out2_loss: 3.9148\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joZRVQCG9ior"
      },
      "source": [
        "y_pred=model.predict(x_test)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYdFnOWR9iqu",
        "outputId": "2b5f80f3-2017-43e4-e643-9e5a678c82c3"
      },
      "source": [
        "len(y_pred)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50ZWgznFSGzo",
        "outputId": "118f62a3-9122-442b-f561-969d12739196"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error as mse\n",
        "print('Error for y1: ', mse(y_pred[0],y1_test))\n",
        "print('Error for y2: ', mse(y_pred[1],y2_test))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error for y1:  4.7529755398735745\n",
            "Error for y2:  7.261465075059144\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZFgwUSyBsDA",
        "outputId": "ba925e32-3cd7-4919-91a2-ed6be058de61"
      },
      "source": [
        "print('Predicted values: ',y_pred[0][:5])\n",
        "print('Actual values: ',y1_test[:5])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted values:  [[11.280081]\n",
            " [39.35876 ]\n",
            " [12.565216]\n",
            " [17.92421 ]\n",
            " [30.873701]]\n",
            "Actual values:  [12.18 40.68 12.25 18.46 32.84]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kv1mvS0yBsHG",
        "outputId": "962593d8-fbce-46bd-f67a-2338b3f70ae4"
      },
      "source": [
        "print('Predicted values: ',y_pred[1][:5])\n",
        "print('Actual values: ',y2_test[:5])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted values:  [[14.364762]\n",
            " [37.5994  ]\n",
            " [15.307201]\n",
            " [20.51624 ]\n",
            " [31.250893]]\n",
            "Actual values:  [15.03 40.36 15.23 21.53 32.88]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFtxFxR_CuX0"
      },
      "source": [
        "Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWq8cVYECwqf",
        "outputId": "9fd7b23a-8d29-44ec-9fd8-978f41d77643"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "his_dict = his.history\n",
        "his_dict.keys()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['loss', 'out1_loss', 'out2_loss'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "S-Nt3tyACwue",
        "outputId": "057a3b8d-189b-4afe-ed86-75651a59b7ab"
      },
      "source": [
        "loss1 = his.history['out1_loss']\n",
        "loss2 = his.history['out2_loss']\n",
        "epochs = list(range(1,len(loss1)+1))\n",
        "plt.plot(epochs,loss1,'b',label='LOSS 1')\n",
        "plt.plot(epochs,loss2,'g',label='LOSS 2')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKJRgHsx_xJK"
      },
      "source": [
        "Since the error corresponding to y2 is very high, we use a different model to train and tune the parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2fnaKswMRjz"
      },
      "source": [
        "Using custom model and custom loss function for model training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlkY7dyPCw52"
      },
      "source": [
        "from keras.layers import Layer\n",
        "\n",
        "# custom dense layer\n",
        "class SimpleDense(Layer):\n",
        "  def __init__(self, units=32, activation= None):  # by default a dense layer of 32 units\n",
        "    super(SimpleDense, self).__init__()\n",
        "    self.units = units\n",
        "    self.activation = tf.keras.activations.get(activation)\n",
        "    \n",
        "  def build(self, input_shape):  # Initializing trainable weights and biases\n",
        "    w_init = tf.random_normal_initializer()\n",
        "    self.w = tf.Variable(name = 'kernel', initial_value = w_init(shape = (input_shape[-1],self.units),dtype = 'float32'), trainable= True)\n",
        "\n",
        "    b_init = tf.zeros_initializer()\n",
        "    self.b = tf.Variable(name = 'bias', initial_value = b_init(shape = (self.units,),dtype = 'float32'), trainable= True)\n",
        "\n",
        "  def call(self, inputs):  # defines computation, called by constructor of the class\n",
        "    return(self.activation(tf.matmul(inputs,self.w)+self.b))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBNiZEm4Mbfl"
      },
      "source": [
        "# custom loss function: huber loss\n",
        "from keras.losses import Loss\n",
        "class myHuberLoss(Loss):\n",
        "  threshold = 1\n",
        "  def __init__(self, threshold):\n",
        "    super().__init__()\n",
        "    self.threshold = threshold\n",
        "  \n",
        "  def call(self,y_true,y_pred):\n",
        "    y_true = tf.cast(y_true, tf.float64)\n",
        "    y_pred = tf.cast(y_pred, tf.float64)\n",
        "    error = y_true-y_pred\n",
        "    is_small_error = tf.abs(error)<=self.threshold\n",
        "    small_error_loss = tf.square(error)*0.1\n",
        "    big_error_loss = self.threshold * (tf.abs(error)- 0.5 * self.threshold)\n",
        "    return tf.where(is_small_error, small_error_loss, big_error_loss)  # (condition, if true, else)\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdOV04PVMbhy"
      },
      "source": [
        "from keras.models import Model\n",
        "\n",
        "def build_model2():\n",
        "  model = Sequential([\n",
        "                      Dense(16, activation='relu', input_shape = (8,)),\n",
        "                      SimpleDense(units=32, activation = 'relu'),\n",
        "                      SimpleDense(units = 64, activation = 'relu'),\n",
        "                      SimpleDense(1)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvKshTKw1Y3-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa1bcab9-dac6-4098-9809-e349a58a82d1"
      },
      "source": [
        "# fitting model and using early stopping technique\n",
        "model2 = build_model2()\n",
        "model2.compile(optimizer= 'adam', loss = myHuberLoss(threshold=0.8))\n",
        "his2 = model2.fit(x_train,y = y2_train, epochs=500, validation_split=0.1, callbacks= [keras.callbacks.EarlyStopping(patience=20, monitor = 'loss', restore_best_weights=True)])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "18/18 [==============================] - 2s 28ms/step - loss: 18.7526 - val_loss: 20.1562\n",
            "Epoch 2/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 19.0391 - val_loss: 19.5087\n",
            "Epoch 3/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 18.0752 - val_loss: 17.1405\n",
            "Epoch 4/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 14.9498 - val_loss: 10.6513\n",
            "Epoch 5/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 8.0900 - val_loss: 4.1892\n",
            "Epoch 6/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 4.3683 - val_loss: 3.8229\n",
            "Epoch 7/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 3.6370 - val_loss: 3.6755\n",
            "Epoch 8/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 3.5716 - val_loss: 3.5256\n",
            "Epoch 9/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 3.3537 - val_loss: 3.4259\n",
            "Epoch 10/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 3.2041 - val_loss: 3.2543\n",
            "Epoch 11/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 2.7978 - val_loss: 3.1945\n",
            "Epoch 12/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 2.8363 - val_loss: 3.0642\n",
            "Epoch 13/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 2.7631 - val_loss: 3.0283\n",
            "Epoch 14/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 2.6857 - val_loss: 2.9357\n",
            "Epoch 15/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 2.7935 - val_loss: 2.8308\n",
            "Epoch 16/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 2.7313 - val_loss: 2.9428\n",
            "Epoch 17/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 2.5290 - val_loss: 2.6760\n",
            "Epoch 18/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 2.5067 - val_loss: 2.6436\n",
            "Epoch 19/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 2.6426 - val_loss: 2.5859\n",
            "Epoch 20/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.2976 - val_loss: 2.4822\n",
            "Epoch 21/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 2.3070 - val_loss: 2.4558\n",
            "Epoch 22/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 2.1345 - val_loss: 2.3441\n",
            "Epoch 23/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 2.0818 - val_loss: 2.3589\n",
            "Epoch 24/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 2.2523 - val_loss: 2.2375\n",
            "Epoch 25/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.8882 - val_loss: 2.2680\n",
            "Epoch 26/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.8341 - val_loss: 2.1890\n",
            "Epoch 27/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.9106 - val_loss: 2.0958\n",
            "Epoch 28/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.8800 - val_loss: 2.1024\n",
            "Epoch 29/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.7471 - val_loss: 2.0429\n",
            "Epoch 30/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.7935 - val_loss: 2.0363\n",
            "Epoch 31/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.7861 - val_loss: 2.0294\n",
            "Epoch 32/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.9371 - val_loss: 1.9014\n",
            "Epoch 33/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.8648 - val_loss: 1.8574\n",
            "Epoch 34/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.6556 - val_loss: 1.8322\n",
            "Epoch 35/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 1.5950 - val_loss: 1.8025\n",
            "Epoch 36/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 1.6473 - val_loss: 1.7499\n",
            "Epoch 37/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 1.7017 - val_loss: 1.7043\n",
            "Epoch 38/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.5029 - val_loss: 1.6799\n",
            "Epoch 39/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.7142 - val_loss: 1.6655\n",
            "Epoch 40/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.6389 - val_loss: 1.6418\n",
            "Epoch 41/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 1.5153 - val_loss: 1.5782\n",
            "Epoch 42/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.5064 - val_loss: 1.5676\n",
            "Epoch 43/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.4192 - val_loss: 1.5414\n",
            "Epoch 44/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.4501 - val_loss: 1.5221\n",
            "Epoch 45/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.5014 - val_loss: 1.5102\n",
            "Epoch 46/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 1.4021 - val_loss: 1.4750\n",
            "Epoch 47/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 1.3934 - val_loss: 1.4437\n",
            "Epoch 48/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.4514 - val_loss: 1.4871\n",
            "Epoch 49/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 1.3488 - val_loss: 1.4085\n",
            "Epoch 50/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.3579 - val_loss: 1.3758\n",
            "Epoch 51/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.4242 - val_loss: 1.3945\n",
            "Epoch 52/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 1.3711 - val_loss: 1.3646\n",
            "Epoch 53/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.3985 - val_loss: 1.3689\n",
            "Epoch 54/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.3041 - val_loss: 1.3239\n",
            "Epoch 55/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.2524 - val_loss: 1.3499\n",
            "Epoch 56/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.2626 - val_loss: 1.3342\n",
            "Epoch 57/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.4331 - val_loss: 1.3800\n",
            "Epoch 58/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.2907 - val_loss: 1.2933\n",
            "Epoch 59/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.2255 - val_loss: 1.3109\n",
            "Epoch 60/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.2980 - val_loss: 1.2982\n",
            "Epoch 61/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.2182 - val_loss: 1.3339\n",
            "Epoch 62/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.1721 - val_loss: 1.2700\n",
            "Epoch 63/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.2000 - val_loss: 1.2709\n",
            "Epoch 64/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.4235 - val_loss: 1.2864\n",
            "Epoch 65/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.3125 - val_loss: 1.2386\n",
            "Epoch 66/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.3330 - val_loss: 1.2665\n",
            "Epoch 67/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.2978 - val_loss: 1.3012\n",
            "Epoch 68/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 1.1794 - val_loss: 1.2364\n",
            "Epoch 69/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 1.2307 - val_loss: 1.2196\n",
            "Epoch 70/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.3218 - val_loss: 1.2237\n",
            "Epoch 71/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 1.2551 - val_loss: 1.2144\n",
            "Epoch 72/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.2098 - val_loss: 1.2296\n",
            "Epoch 73/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.3345 - val_loss: 1.2128\n",
            "Epoch 74/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.2561 - val_loss: 1.2243\n",
            "Epoch 75/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.1516 - val_loss: 1.2251\n",
            "Epoch 76/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.3923 - val_loss: 1.2063\n",
            "Epoch 77/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.2333 - val_loss: 1.2201\n",
            "Epoch 78/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.3061 - val_loss: 1.2182\n",
            "Epoch 79/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.3265 - val_loss: 1.2371\n",
            "Epoch 80/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.2978 - val_loss: 1.1843\n",
            "Epoch 81/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.2618 - val_loss: 1.1747\n",
            "Epoch 82/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.2522 - val_loss: 1.2313\n",
            "Epoch 83/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 1.2577 - val_loss: 1.1932\n",
            "Epoch 84/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.1030 - val_loss: 1.1718\n",
            "Epoch 85/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.2038 - val_loss: 1.1680\n",
            "Epoch 86/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.1553 - val_loss: 1.2360\n",
            "Epoch 87/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.2282 - val_loss: 1.1797\n",
            "Epoch 88/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.1156 - val_loss: 1.2206\n",
            "Epoch 89/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.2764 - val_loss: 1.1415\n",
            "Epoch 90/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.2345 - val_loss: 1.2914\n",
            "Epoch 91/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 1.2352 - val_loss: 1.1201\n",
            "Epoch 92/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.1728 - val_loss: 1.1447\n",
            "Epoch 93/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 1.1208 - val_loss: 1.1297\n",
            "Epoch 94/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.2073 - val_loss: 1.1751\n",
            "Epoch 95/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 1.1911 - val_loss: 1.1229\n",
            "Epoch 96/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 1.1744 - val_loss: 1.0973\n",
            "Epoch 97/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 1.3047 - val_loss: 1.1646\n",
            "Epoch 98/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.2112 - val_loss: 1.1845\n",
            "Epoch 99/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.1845 - val_loss: 1.1677\n",
            "Epoch 100/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.1359 - val_loss: 1.0829\n",
            "Epoch 101/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.2143 - val_loss: 1.0649\n",
            "Epoch 102/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.2645 - val_loss: 1.1261\n",
            "Epoch 103/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.2009 - val_loss: 1.2496\n",
            "Epoch 104/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.2087 - val_loss: 1.0462\n",
            "Epoch 105/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.2839 - val_loss: 1.0467\n",
            "Epoch 106/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.0032 - val_loss: 1.0350\n",
            "Epoch 107/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.0807 - val_loss: 1.0387\n",
            "Epoch 108/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.1764 - val_loss: 1.0059\n",
            "Epoch 109/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.1445 - val_loss: 0.9915\n",
            "Epoch 110/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.1438 - val_loss: 1.0091\n",
            "Epoch 111/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.0182 - val_loss: 0.9686\n",
            "Epoch 112/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.0318 - val_loss: 0.9940\n",
            "Epoch 113/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.1228 - val_loss: 0.9837\n",
            "Epoch 114/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.9716 - val_loss: 0.9315\n",
            "Epoch 115/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.0575 - val_loss: 0.9637\n",
            "Epoch 116/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.1223 - val_loss: 0.9420\n",
            "Epoch 117/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.1097 - val_loss: 0.9263\n",
            "Epoch 118/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.0392 - val_loss: 0.9087\n",
            "Epoch 119/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.0137 - val_loss: 0.8777\n",
            "Epoch 120/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 1.0754 - val_loss: 0.8883\n",
            "Epoch 121/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.0157 - val_loss: 0.9409\n",
            "Epoch 122/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.1284 - val_loss: 0.8461\n",
            "Epoch 123/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0144 - val_loss: 0.8572\n",
            "Epoch 124/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 1.0033 - val_loss: 0.8709\n",
            "Epoch 125/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.0137 - val_loss: 0.8842\n",
            "Epoch 126/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.0269 - val_loss: 0.8494\n",
            "Epoch 127/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.0071 - val_loss: 0.8317\n",
            "Epoch 128/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.0124 - val_loss: 0.8409\n",
            "Epoch 129/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.9885 - val_loss: 0.8127\n",
            "Epoch 130/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.9162 - val_loss: 0.8329\n",
            "Epoch 131/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.0376 - val_loss: 0.7914\n",
            "Epoch 132/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 1.0164 - val_loss: 0.7905\n",
            "Epoch 133/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.9431 - val_loss: 0.8120\n",
            "Epoch 134/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 1.0600 - val_loss: 0.7801\n",
            "Epoch 135/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 1.0007 - val_loss: 0.8735\n",
            "Epoch 136/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.0908 - val_loss: 0.7697\n",
            "Epoch 137/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.0482 - val_loss: 0.7744\n",
            "Epoch 138/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.9605 - val_loss: 0.7630\n",
            "Epoch 139/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.9607 - val_loss: 0.7937\n",
            "Epoch 140/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.9269 - val_loss: 0.7672\n",
            "Epoch 141/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.9899 - val_loss: 0.7478\n",
            "Epoch 142/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 1.0157 - val_loss: 0.7621\n",
            "Epoch 143/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.9253 - val_loss: 0.7390\n",
            "Epoch 144/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.9993 - val_loss: 0.7626\n",
            "Epoch 145/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.9105 - val_loss: 0.7460\n",
            "Epoch 146/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8950 - val_loss: 0.7382\n",
            "Epoch 147/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.9374 - val_loss: 0.7240\n",
            "Epoch 148/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.9983 - val_loss: 0.8130\n",
            "Epoch 149/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.0572 - val_loss: 0.7356\n",
            "Epoch 150/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.0707 - val_loss: 0.7437\n",
            "Epoch 151/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.9292 - val_loss: 0.7658\n",
            "Epoch 152/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.0194 - val_loss: 0.7818\n",
            "Epoch 153/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.9491 - val_loss: 0.7824\n",
            "Epoch 154/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.9917 - val_loss: 0.7386\n",
            "Epoch 155/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.9386 - val_loss: 0.7374\n",
            "Epoch 156/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 1.0032 - val_loss: 0.7133\n",
            "Epoch 157/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.9053 - val_loss: 0.7345\n",
            "Epoch 158/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.9079 - val_loss: 0.7273\n",
            "Epoch 159/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.9570 - val_loss: 0.7753\n",
            "Epoch 160/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.0671 - val_loss: 0.7280\n",
            "Epoch 161/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8712 - val_loss: 0.7436\n",
            "Epoch 162/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.0237 - val_loss: 0.7896\n",
            "Epoch 163/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 1.0125 - val_loss: 0.7409\n",
            "Epoch 164/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.9348 - val_loss: 0.7022\n",
            "Epoch 165/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 1.0134 - val_loss: 0.7166\n",
            "Epoch 166/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.9664 - val_loss: 0.7541\n",
            "Epoch 167/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.9415 - val_loss: 0.7170\n",
            "Epoch 168/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.0158 - val_loss: 0.7957\n",
            "Epoch 169/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.9861 - val_loss: 0.7137\n",
            "Epoch 170/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8599 - val_loss: 0.7307\n",
            "Epoch 171/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.9305 - val_loss: 0.7240\n",
            "Epoch 172/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.9166 - val_loss: 0.7191\n",
            "Epoch 173/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.9583 - val_loss: 0.7575\n",
            "Epoch 174/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.0227 - val_loss: 0.6983\n",
            "Epoch 175/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.9076 - val_loss: 0.6962\n",
            "Epoch 176/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.9551 - val_loss: 0.7252\n",
            "Epoch 177/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8730 - val_loss: 0.7120\n",
            "Epoch 178/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8738 - val_loss: 0.7184\n",
            "Epoch 179/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8809 - val_loss: 0.7213\n",
            "Epoch 180/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.0158 - val_loss: 0.7782\n",
            "Epoch 181/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.9312 - val_loss: 0.7256\n",
            "Epoch 182/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.9372 - val_loss: 0.7697\n",
            "Epoch 183/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8664 - val_loss: 0.6719\n",
            "Epoch 184/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.9273 - val_loss: 0.7217\n",
            "Epoch 185/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8758 - val_loss: 0.7096\n",
            "Epoch 186/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.9122 - val_loss: 0.7209\n",
            "Epoch 187/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.9958 - val_loss: 0.7583\n",
            "Epoch 188/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.9076 - val_loss: 0.7931\n",
            "Epoch 189/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.9941 - val_loss: 0.7296\n",
            "Epoch 190/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.9583 - val_loss: 0.7422\n",
            "Epoch 191/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8704 - val_loss: 0.7175\n",
            "Epoch 192/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8882 - val_loss: 0.6786\n",
            "Epoch 193/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.9599 - val_loss: 0.7086\n",
            "Epoch 194/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.0387 - val_loss: 0.6932\n",
            "Epoch 195/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.9681 - val_loss: 0.7212\n",
            "Epoch 196/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8458 - val_loss: 0.7221\n",
            "Epoch 197/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8838 - val_loss: 0.7322\n",
            "Epoch 198/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.9292 - val_loss: 0.7115\n",
            "Epoch 199/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8959 - val_loss: 0.6666\n",
            "Epoch 200/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8082 - val_loss: 0.7594\n",
            "Epoch 201/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.0137 - val_loss: 0.6714\n",
            "Epoch 202/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8994 - val_loss: 0.6832\n",
            "Epoch 203/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.9080 - val_loss: 0.7373\n",
            "Epoch 204/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8957 - val_loss: 0.6711\n",
            "Epoch 205/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8525 - val_loss: 0.7570\n",
            "Epoch 206/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.9464 - val_loss: 0.7075\n",
            "Epoch 207/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.0180 - val_loss: 0.7074\n",
            "Epoch 208/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 1.0392 - val_loss: 0.6822\n",
            "Epoch 209/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.8601 - val_loss: 0.6885\n",
            "Epoch 210/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0550 - val_loss: 0.7043\n",
            "Epoch 211/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8744 - val_loss: 0.7035\n",
            "Epoch 212/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.9561 - val_loss: 0.7584\n",
            "Epoch 213/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8864 - val_loss: 0.6859\n",
            "Epoch 214/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.9181 - val_loss: 0.7421\n",
            "Epoch 215/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8579 - val_loss: 0.6341\n",
            "Epoch 216/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.9350 - val_loss: 0.6946\n",
            "Epoch 217/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8202 - val_loss: 0.7239\n",
            "Epoch 218/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.9748 - val_loss: 0.7026\n",
            "Epoch 219/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.9250 - val_loss: 0.6722\n",
            "Epoch 220/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7976 - val_loss: 0.6566\n",
            "Epoch 221/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8897 - val_loss: 0.8399\n",
            "Epoch 222/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.0144 - val_loss: 0.7644\n",
            "Epoch 223/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.9376 - val_loss: 0.6893\n",
            "Epoch 224/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8817 - val_loss: 0.6813\n",
            "Epoch 225/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.9553 - val_loss: 0.6720\n",
            "Epoch 226/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.0407 - val_loss: 0.6700\n",
            "Epoch 227/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8189 - val_loss: 0.6895\n",
            "Epoch 228/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8567 - val_loss: 0.6891\n",
            "Epoch 229/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8894 - val_loss: 0.6392\n",
            "Epoch 230/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.9427 - val_loss: 0.6412\n",
            "Epoch 231/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8722 - val_loss: 0.6764\n",
            "Epoch 232/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8785 - val_loss: 0.6613\n",
            "Epoch 233/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8606 - val_loss: 0.6169\n",
            "Epoch 234/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8520 - val_loss: 0.6583\n",
            "Epoch 235/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.7756 - val_loss: 0.6720\n",
            "Epoch 236/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.9440 - val_loss: 0.6568\n",
            "Epoch 237/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8519 - val_loss: 0.6646\n",
            "Epoch 238/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8807 - val_loss: 0.6864\n",
            "Epoch 239/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.9793 - val_loss: 0.6672\n",
            "Epoch 240/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.9052 - val_loss: 0.6440\n",
            "Epoch 241/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7609 - val_loss: 0.6696\n",
            "Epoch 242/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.9445 - val_loss: 0.6368\n",
            "Epoch 243/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8764 - val_loss: 0.6212\n",
            "Epoch 244/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8360 - val_loss: 0.6345\n",
            "Epoch 245/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8725 - val_loss: 0.6215\n",
            "Epoch 246/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8631 - val_loss: 0.7028\n",
            "Epoch 247/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.9331 - val_loss: 0.6815\n",
            "Epoch 248/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.8422 - val_loss: 0.6153\n",
            "Epoch 249/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8714 - val_loss: 0.6260\n",
            "Epoch 250/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8307 - val_loss: 0.6884\n",
            "Epoch 251/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8355 - val_loss: 0.6222\n",
            "Epoch 252/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7920 - val_loss: 0.5982\n",
            "Epoch 253/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8751 - val_loss: 0.6013\n",
            "Epoch 254/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8013 - val_loss: 0.5982\n",
            "Epoch 255/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7946 - val_loss: 0.6645\n",
            "Epoch 256/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.9049 - val_loss: 0.5962\n",
            "Epoch 257/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.8122 - val_loss: 0.6585\n",
            "Epoch 258/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8390 - val_loss: 0.5898\n",
            "Epoch 259/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.8820 - val_loss: 0.6188\n",
            "Epoch 260/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8809 - val_loss: 0.6015\n",
            "Epoch 261/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7348 - val_loss: 0.5937\n",
            "Epoch 262/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7937 - val_loss: 0.5819\n",
            "Epoch 263/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7929 - val_loss: 0.6280\n",
            "Epoch 264/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8281 - val_loss: 0.5939\n",
            "Epoch 265/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6944 - val_loss: 0.5781\n",
            "Epoch 266/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8723 - val_loss: 0.5759\n",
            "Epoch 267/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8134 - val_loss: 0.7213\n",
            "Epoch 268/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8555 - val_loss: 0.5860\n",
            "Epoch 269/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8325 - val_loss: 0.5613\n",
            "Epoch 270/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7764 - val_loss: 0.5479\n",
            "Epoch 271/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8037 - val_loss: 0.5740\n",
            "Epoch 272/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8110 - val_loss: 0.5712\n",
            "Epoch 273/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7819 - val_loss: 0.5535\n",
            "Epoch 274/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7798 - val_loss: 0.5388\n",
            "Epoch 275/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.7952 - val_loss: 0.5401\n",
            "Epoch 276/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7138 - val_loss: 0.5712\n",
            "Epoch 277/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7565 - val_loss: 0.5426\n",
            "Epoch 278/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6766 - val_loss: 0.5375\n",
            "Epoch 279/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.7344 - val_loss: 0.5697\n",
            "Epoch 280/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8268 - val_loss: 0.5451\n",
            "Epoch 281/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7702 - val_loss: 0.5784\n",
            "Epoch 282/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7419 - val_loss: 0.6405\n",
            "Epoch 283/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8486 - val_loss: 0.7097\n",
            "Epoch 284/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7592 - val_loss: 0.5357\n",
            "Epoch 285/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7314 - val_loss: 0.5636\n",
            "Epoch 286/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7825 - val_loss: 0.5159\n",
            "Epoch 287/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7344 - val_loss: 0.5543\n",
            "Epoch 288/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7293 - val_loss: 0.5357\n",
            "Epoch 289/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7241 - val_loss: 0.5295\n",
            "Epoch 290/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6920 - val_loss: 0.5328\n",
            "Epoch 291/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7152 - val_loss: 0.5710\n",
            "Epoch 292/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6895 - val_loss: 0.5872\n",
            "Epoch 293/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7517 - val_loss: 0.5316\n",
            "Epoch 294/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6506 - val_loss: 0.5233\n",
            "Epoch 295/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.7576 - val_loss: 0.5485\n",
            "Epoch 296/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7191 - val_loss: 0.5397\n",
            "Epoch 297/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.5466\n",
            "Epoch 298/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.8529 - val_loss: 0.5978\n",
            "Epoch 299/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8017 - val_loss: 0.5188\n",
            "Epoch 300/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7167 - val_loss: 0.5168\n",
            "Epoch 301/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6401 - val_loss: 0.5281\n",
            "Epoch 302/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6821 - val_loss: 0.5305\n",
            "Epoch 303/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7302 - val_loss: 0.5798\n",
            "Epoch 304/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6975 - val_loss: 0.5106\n",
            "Epoch 305/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6509 - val_loss: 0.5069\n",
            "Epoch 306/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7605 - val_loss: 0.5348\n",
            "Epoch 307/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7247 - val_loss: 0.5420\n",
            "Epoch 308/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7336 - val_loss: 0.5028\n",
            "Epoch 309/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6346 - val_loss: 0.5226\n",
            "Epoch 310/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7718 - val_loss: 0.5078\n",
            "Epoch 311/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6963 - val_loss: 0.4915\n",
            "Epoch 312/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7345 - val_loss: 0.5277\n",
            "Epoch 313/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7593 - val_loss: 0.5238\n",
            "Epoch 314/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6427 - val_loss: 0.4898\n",
            "Epoch 315/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6951 - val_loss: 0.5178\n",
            "Epoch 316/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7123 - val_loss: 0.5411\n",
            "Epoch 317/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7908 - val_loss: 0.5462\n",
            "Epoch 318/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7428 - val_loss: 0.4955\n",
            "Epoch 319/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6765 - val_loss: 0.5133\n",
            "Epoch 320/500\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.7888 - val_loss: 0.4987\n",
            "Epoch 321/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7115 - val_loss: 0.5382\n",
            "Epoch 322/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7326 - val_loss: 0.5245\n",
            "Epoch 323/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6983 - val_loss: 0.5348\n",
            "Epoch 324/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6567 - val_loss: 0.5006\n",
            "Epoch 325/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7058 - val_loss: 0.5246\n",
            "Epoch 326/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6810 - val_loss: 0.5231\n",
            "Epoch 327/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6664 - val_loss: 0.5168\n",
            "Epoch 328/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6920 - val_loss: 0.5353\n",
            "Epoch 329/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6899 - val_loss: 0.5107\n",
            "Epoch 330/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6963 - val_loss: 0.5332\n",
            "Epoch 331/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7158 - val_loss: 0.5072\n",
            "Epoch 332/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7332 - val_loss: 0.6121\n",
            "Epoch 333/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6478 - val_loss: 0.5193\n",
            "Epoch 334/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6596 - val_loss: 0.4978\n",
            "Epoch 335/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7861 - val_loss: 0.6030\n",
            "Epoch 336/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7145 - val_loss: 0.5179\n",
            "Epoch 337/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6587 - val_loss: 0.5249\n",
            "Epoch 338/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6582 - val_loss: 0.5247\n",
            "Epoch 339/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6649 - val_loss: 0.5418\n",
            "Epoch 340/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6922 - val_loss: 0.5508\n",
            "Epoch 341/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6620 - val_loss: 0.5267\n",
            "Epoch 342/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6921 - val_loss: 0.5535\n",
            "Epoch 343/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6892 - val_loss: 0.5643\n",
            "Epoch 344/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7173 - val_loss: 0.5678\n",
            "Epoch 345/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6369 - val_loss: 0.5380\n",
            "Epoch 346/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6377 - val_loss: 0.5642\n",
            "Epoch 347/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7267 - val_loss: 0.5673\n",
            "Epoch 348/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7131 - val_loss: 0.5414\n",
            "Epoch 349/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6079 - val_loss: 0.5448\n",
            "Epoch 350/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7432 - val_loss: 0.5875\n",
            "Epoch 351/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6633 - val_loss: 0.5363\n",
            "Epoch 352/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6473 - val_loss: 0.5598\n",
            "Epoch 353/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6847 - val_loss: 0.5319\n",
            "Epoch 354/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5783 - val_loss: 0.5613\n",
            "Epoch 355/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6796 - val_loss: 0.5771\n",
            "Epoch 356/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6351 - val_loss: 0.5742\n",
            "Epoch 357/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6386 - val_loss: 0.5335\n",
            "Epoch 358/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6104 - val_loss: 0.5362\n",
            "Epoch 359/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7067 - val_loss: 0.6068\n",
            "Epoch 360/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6821 - val_loss: 0.5399\n",
            "Epoch 361/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7089 - val_loss: 0.5440\n",
            "Epoch 362/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6482 - val_loss: 0.5357\n",
            "Epoch 363/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6847 - val_loss: 0.5861\n",
            "Epoch 364/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6371 - val_loss: 0.5504\n",
            "Epoch 365/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6426 - val_loss: 0.5537\n",
            "Epoch 366/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6707 - val_loss: 0.5593\n",
            "Epoch 367/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6326 - val_loss: 0.5756\n",
            "Epoch 368/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7298 - val_loss: 0.6189\n",
            "Epoch 369/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6634 - val_loss: 0.5458\n",
            "Epoch 370/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6980 - val_loss: 0.6411\n",
            "Epoch 371/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6872 - val_loss: 0.5780\n",
            "Epoch 372/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6621 - val_loss: 0.5768\n",
            "Epoch 373/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7005 - val_loss: 0.6655\n",
            "Epoch 374/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6528 - val_loss: 0.5522\n",
            "Epoch 375/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6866 - val_loss: 0.5666\n",
            "Epoch 376/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6914 - val_loss: 0.5740\n",
            "Epoch 377/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6091 - val_loss: 0.5566\n",
            "Epoch 378/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6848 - val_loss: 0.5492\n",
            "Epoch 379/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6645 - val_loss: 0.5541\n",
            "Epoch 380/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6636 - val_loss: 0.5603\n",
            "Epoch 381/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6264 - val_loss: 0.5619\n",
            "Epoch 382/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6255 - val_loss: 0.6220\n",
            "Epoch 383/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.7097 - val_loss: 0.5683\n",
            "Epoch 384/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6557 - val_loss: 0.5740\n",
            "Epoch 385/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6452 - val_loss: 0.6059\n",
            "Epoch 386/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7721 - val_loss: 0.6307\n",
            "Epoch 387/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6695 - val_loss: 0.5807\n",
            "Epoch 388/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6328 - val_loss: 0.5841\n",
            "Epoch 389/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6101 - val_loss: 0.5854\n",
            "Epoch 390/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6386 - val_loss: 0.5650\n",
            "Epoch 391/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7246 - val_loss: 0.6550\n",
            "Epoch 392/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5989 - val_loss: 0.5984\n",
            "Epoch 393/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6860 - val_loss: 0.5979\n",
            "Epoch 394/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5976 - val_loss: 0.6354\n",
            "Epoch 395/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7651 - val_loss: 0.5781\n",
            "Epoch 396/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6223 - val_loss: 0.5872\n",
            "Epoch 397/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6649 - val_loss: 0.5579\n",
            "Epoch 398/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6485 - val_loss: 0.5798\n",
            "Epoch 399/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6773 - val_loss: 0.6383\n",
            "Epoch 400/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5974 - val_loss: 0.5816\n",
            "Epoch 401/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7083 - val_loss: 0.6264\n",
            "Epoch 402/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6526 - val_loss: 0.5979\n",
            "Epoch 403/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7118 - val_loss: 0.6211\n",
            "Epoch 404/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6782 - val_loss: 0.6401\n",
            "Epoch 405/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6609 - val_loss: 0.6037\n",
            "Epoch 406/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6297 - val_loss: 0.5839\n",
            "Epoch 407/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6503 - val_loss: 0.5939\n",
            "Epoch 408/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6659 - val_loss: 0.6064\n",
            "Epoch 409/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5769 - val_loss: 0.5978\n",
            "Epoch 410/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6387 - val_loss: 0.5895\n",
            "Epoch 411/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6146 - val_loss: 0.5975\n",
            "Epoch 412/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7050 - val_loss: 0.6031\n",
            "Epoch 413/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6197 - val_loss: 0.6286\n",
            "Epoch 414/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6481 - val_loss: 0.6068\n",
            "Epoch 415/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6266 - val_loss: 0.5986\n",
            "Epoch 416/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5929 - val_loss: 0.7323\n",
            "Epoch 417/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6192 - val_loss: 0.6649\n",
            "Epoch 418/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6868 - val_loss: 0.5912\n",
            "Epoch 419/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6380 - val_loss: 0.6276\n",
            "Epoch 420/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7193 - val_loss: 0.6089\n",
            "Epoch 421/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6041 - val_loss: 0.6102\n",
            "Epoch 422/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5769 - val_loss: 0.6141\n",
            "Epoch 423/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6124 - val_loss: 0.5892\n",
            "Epoch 424/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6241 - val_loss: 0.6247\n",
            "Epoch 425/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6551 - val_loss: 0.6327\n",
            "Epoch 426/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6236 - val_loss: 0.5964\n",
            "Epoch 427/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5077 - val_loss: 0.6169\n",
            "Epoch 428/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6263 - val_loss: 0.6122\n",
            "Epoch 429/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5938 - val_loss: 0.6845\n",
            "Epoch 430/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5545 - val_loss: 0.5972\n",
            "Epoch 431/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6652 - val_loss: 0.5926\n",
            "Epoch 432/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5976 - val_loss: 0.6126\n",
            "Epoch 433/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6507 - val_loss: 0.6473\n",
            "Epoch 434/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5677 - val_loss: 0.6154\n",
            "Epoch 435/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6706 - val_loss: 0.6090\n",
            "Epoch 436/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5918 - val_loss: 0.5919\n",
            "Epoch 437/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5721 - val_loss: 0.6841\n",
            "Epoch 438/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6577 - val_loss: 0.6441\n",
            "Epoch 439/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6467 - val_loss: 0.6400\n",
            "Epoch 440/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6062 - val_loss: 0.6757\n",
            "Epoch 441/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5409 - val_loss: 0.6429\n",
            "Epoch 442/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6342 - val_loss: 0.5808\n",
            "Epoch 443/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5784 - val_loss: 0.6201\n",
            "Epoch 444/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5745 - val_loss: 0.5845\n",
            "Epoch 445/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6061 - val_loss: 0.5811\n",
            "Epoch 446/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5625 - val_loss: 0.6093\n",
            "Epoch 447/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6261 - val_loss: 0.5957\n",
            "Epoch 448/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5675 - val_loss: 0.6101\n",
            "Epoch 449/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5526 - val_loss: 0.6780\n",
            "Epoch 450/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5928 - val_loss: 0.6282\n",
            "Epoch 451/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5741 - val_loss: 0.5915\n",
            "Epoch 452/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5301 - val_loss: 0.6384\n",
            "Epoch 453/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5435 - val_loss: 0.6231\n",
            "Epoch 454/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5808 - val_loss: 0.5896\n",
            "Epoch 455/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5437 - val_loss: 0.5796\n",
            "Epoch 456/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5706 - val_loss: 0.5967\n",
            "Epoch 457/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5999 - val_loss: 0.5979\n",
            "Epoch 458/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5907 - val_loss: 0.5798\n",
            "Epoch 459/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4881 - val_loss: 0.5899\n",
            "Epoch 460/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5481 - val_loss: 0.5792\n",
            "Epoch 461/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4821 - val_loss: 0.5910\n",
            "Epoch 462/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5463 - val_loss: 0.6096\n",
            "Epoch 463/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5276 - val_loss: 0.5759\n",
            "Epoch 464/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5080 - val_loss: 0.5856\n",
            "Epoch 465/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5532 - val_loss: 0.5982\n",
            "Epoch 466/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5710 - val_loss: 0.5819\n",
            "Epoch 467/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5384 - val_loss: 0.6122\n",
            "Epoch 468/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6118 - val_loss: 0.6027\n",
            "Epoch 469/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4927 - val_loss: 0.5867\n",
            "Epoch 470/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4866 - val_loss: 0.5864\n",
            "Epoch 471/500\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5298 - val_loss: 0.5909\n",
            "Epoch 472/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5103 - val_loss: 0.5838\n",
            "Epoch 473/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5481 - val_loss: 0.6051\n",
            "Epoch 474/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4951 - val_loss: 0.6297\n",
            "Epoch 475/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5844 - val_loss: 0.6050\n",
            "Epoch 476/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5087 - val_loss: 0.6141\n",
            "Epoch 477/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5207 - val_loss: 0.5786\n",
            "Epoch 478/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5208 - val_loss: 0.6127\n",
            "Epoch 479/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5405 - val_loss: 0.6307\n",
            "Epoch 480/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4931 - val_loss: 0.5708\n",
            "Epoch 481/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5119 - val_loss: 0.6737\n",
            "Epoch 482/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5557 - val_loss: 0.5694\n",
            "Epoch 483/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5397 - val_loss: 0.5803\n",
            "Epoch 484/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5319 - val_loss: 0.5878\n",
            "Epoch 485/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4382 - val_loss: 0.6187\n",
            "Epoch 486/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5091 - val_loss: 0.6040\n",
            "Epoch 487/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5388 - val_loss: 0.5964\n",
            "Epoch 488/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5416 - val_loss: 0.6228\n",
            "Epoch 489/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4800 - val_loss: 0.5998\n",
            "Epoch 490/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5885 - val_loss: 0.6227\n",
            "Epoch 491/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4994 - val_loss: 0.5855\n",
            "Epoch 492/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5042 - val_loss: 0.5877\n",
            "Epoch 493/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4321 - val_loss: 0.5944\n",
            "Epoch 494/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5318 - val_loss: 0.6730\n",
            "Epoch 495/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5163 - val_loss: 0.6025\n",
            "Epoch 496/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4972 - val_loss: 0.6113\n",
            "Epoch 497/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4858 - val_loss: 0.6707\n",
            "Epoch 498/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5609 - val_loss: 0.5874\n",
            "Epoch 499/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4818 - val_loss: 0.5804\n",
            "Epoch 500/500\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5173 - val_loss: 0.5886\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lGkkuhCMbj9"
      },
      "source": [
        "y_pred2 = model2.predict(x_test)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Lk740Gh6R27",
        "outputId": "8d574bac-27c3-4750-b10f-dfab0a69bf46"
      },
      "source": [
        "# Calculating the mean squared error\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "print('Error for y2: ', mse(y_pred2,y2_test))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error for y2:  4.210141322480229\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ai4jM1pfCvy6",
        "outputId": "f36373eb-dde4-4e78-b7b8-9bf45e9d9c26"
      },
      "source": [
        "print('Predicted values: ',y_pred2[:5])\n",
        "print('Actual values: ', y2_test[:5])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted values:  [[14.896695]\n",
            " [39.158104]\n",
            " [14.017791]\n",
            " [21.98635 ]\n",
            " [33.16597 ]]\n",
            "Actual values:  [15.03 40.36 15.23 21.53 32.88]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-D4enJ0a6uLg"
      },
      "source": [
        "Visualisation of loss vs epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waTrldgb6R5M",
        "outputId": "5b65a510-bf73-4940-94cc-f82491453962"
      },
      "source": [
        "his2.history.keys()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['loss', 'val_loss'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "lGbVG-6367JZ",
        "outputId": "8a2c5020-14de-404c-c78c-b8356995628f"
      },
      "source": [
        "loss2 = his2.history['loss']\n",
        "val_loss = his2.history['val_loss']\n",
        "epochs = list(range(1,len(loss2)+1))\n",
        "plt.plot(epochs,loss2,'b',label='LOSS model 2')\n",
        "plt.plot(epochs,val_loss,'go',label='validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-r322Y4EDSoI"
      },
      "source": [
        "Some small changes in loss fucntion improved the performance of the model. The updated huber loss function reduced the speed of updating weights and biases for smaller loss.\\\n",
        "Early stopping was used to restore the best parameters to train the model.\n",
        "\n",
        "Functional API was implemented, losses were compared. A custom dense layer was designed. The custom huber loss function improved the model performance by some extent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YO6kGUYibyp_"
      },
      "source": [
        "**Training model for y1.**\\\n",
        "Implementing model compilation and fitting from scratch.\\\n",
        "Use of custom huber loss function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cnQ3vGObxxC"
      },
      "source": [
        "model1 = Sequential([\n",
        "                      Dense(16, activation='relu', input_shape = (8,)),\n",
        "                      Dense(units=32, activation = 'relu'),\n",
        "                      Dense(units = 64, activation = 'relu'),\n",
        "                      Dense(1)\n",
        "  ])"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zz83mExbb4U"
      },
      "source": [
        "# applying gradient on batch of dataset\n",
        "#myloss = myHuberLoss(threshold = 0.8)\n",
        "def apply_gradient(optimizer, model1, x,y):\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = model1(x)\n",
        "    loss_value = tf.keras.losses.MSE(y, logits)\n",
        "  gradients = tape.gradient(loss_value, model1.trainable_weights)\n",
        "  optimizer.apply_gradients(zip(gradients, model1.trainable_weights))\n",
        "  return logits,loss_value"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QalL6ffscxpi"
      },
      "source": [
        "def perform_validation():\n",
        "  losses = []\n",
        "  for batch in test:\n",
        "    x_val, y_val = batch[:,:8],batch[:,8]\n",
        "    val_logits = model1(x_val)\n",
        "    val_loss = tf.keras.losses.MSE(y_val, val_logits)\n",
        "    losses.append(val_loss)\n",
        "  return losses"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8CdO0idcxr8"
      },
      "source": [
        "def train_data_for_one_epoch():\n",
        "  losses = []\n",
        "  for batch in train:\n",
        "    x_batch_train, y_batch_train = batch[:,:8],batch[:,8]\n",
        "    #print(x_batch_train.shape, y_batch_train.shape)\n",
        "    logits, loss = apply_gradient(optimizer,model1,x_batch_train, y_batch_train)\n",
        "    losses.append(loss)\n",
        "  return losses"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrO6hlZ1f0GM",
        "outputId": "f57ef08f-0be2-4b42-e9ea-382107b6b55d"
      },
      "source": [
        "# preparing dataset to yeild batches of examples\n",
        "train_dataset = pd.DataFrame(x_train)\n",
        "train_dataset['target'] = y1_train\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_dataset)\n",
        "test_dataset = pd.DataFrame(x_test)\n",
        "test_dataset['target'] = y1_test\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(test_dataset)\n",
        "train = train_dataset.shuffle(len(y1_train)).repeat(2).batch(64,drop_remainder = True)\n",
        "test = test_dataset.batch(64,drop_remainder=True)\n",
        "train,test"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<BatchDataset shapes: (64, 9), types: tf.float64>,\n",
              " <BatchDataset shapes: (64, 9), types: tf.float64>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rr-0seTIf0zu",
        "outputId": "902432d8-24dc-4537-f5dd-f7f227d8b8d9"
      },
      "source": [
        "epochs = 50\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
        "val_losses, train_losses = [],[]\n",
        "for epoch in range(epochs):\n",
        "  print('Start of %d epoch'%(epoch+1))\n",
        "\n",
        "  losses_train = train_data_for_one_epoch()\n",
        "  losses_val = perform_validation()\n",
        "\n",
        "  train_losses.append(np.mean(losses_train))\n",
        "  val_losses.append(np.mean(losses_val))\n",
        "\n",
        "  print('\\t Train loss : {}, val loss : {}'.format(train_losses[-1],val_losses[-1]))\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start of 1 epoch\n",
            "\t Train loss : 578.8919677734375, val loss : 616.2180786132812\n",
            "Start of 2 epoch\n",
            "\t Train loss : 551.8404541015625, val loss : 582.7156982421875\n",
            "Start of 3 epoch\n",
            "\t Train loss : 512.9132080078125, val loss : 533.9613647460938\n",
            "Start of 4 epoch\n",
            "\t Train loss : 457.8067321777344, val loss : 460.697509765625\n",
            "Start of 5 epoch\n",
            "\t Train loss : 372.1768493652344, val loss : 358.4390869140625\n",
            "Start of 6 epoch\n",
            "\t Train loss : 266.4537353515625, val loss : 245.21237182617188\n",
            "Start of 7 epoch\n",
            "\t Train loss : 174.31414794921875, val loss : 164.1023712158203\n",
            "Start of 8 epoch\n",
            "\t Train loss : 129.5884246826172, val loss : 138.18157958984375\n",
            "Start of 9 epoch\n",
            "\t Train loss : 122.47525024414062, val loss : 132.7157745361328\n",
            "Start of 10 epoch\n",
            "\t Train loss : 119.4693832397461, val loss : 129.87002563476562\n",
            "Start of 11 epoch\n",
            "\t Train loss : 116.41510009765625, val loss : 127.13402557373047\n",
            "Start of 12 epoch\n",
            "\t Train loss : 114.51850128173828, val loss : 124.3623046875\n",
            "Start of 13 epoch\n",
            "\t Train loss : 113.29270935058594, val loss : 122.64119720458984\n",
            "Start of 14 epoch\n",
            "\t Train loss : 112.2977523803711, val loss : 120.93910217285156\n",
            "Start of 15 epoch\n",
            "\t Train loss : 110.49130249023438, val loss : 119.31727600097656\n",
            "Start of 16 epoch\n",
            "\t Train loss : 109.55892944335938, val loss : 118.73466491699219\n",
            "Start of 17 epoch\n",
            "\t Train loss : 109.2971420288086, val loss : 117.924560546875\n",
            "Start of 18 epoch\n",
            "\t Train loss : 108.06108856201172, val loss : 116.55599975585938\n",
            "Start of 19 epoch\n",
            "\t Train loss : 108.07049560546875, val loss : 116.16094970703125\n",
            "Start of 20 epoch\n",
            "\t Train loss : 107.36279296875, val loss : 115.16799926757812\n",
            "Start of 21 epoch\n",
            "\t Train loss : 107.02874755859375, val loss : 115.19524383544922\n",
            "Start of 22 epoch\n",
            "\t Train loss : 106.41902160644531, val loss : 114.03355407714844\n",
            "Start of 23 epoch\n",
            "\t Train loss : 106.03900909423828, val loss : 114.01480102539062\n",
            "Start of 24 epoch\n",
            "\t Train loss : 105.81145477294922, val loss : 113.49403381347656\n",
            "Start of 25 epoch\n",
            "\t Train loss : 105.82380676269531, val loss : 113.11046600341797\n",
            "Start of 26 epoch\n",
            "\t Train loss : 105.24810791015625, val loss : 112.76101684570312\n",
            "Start of 27 epoch\n",
            "\t Train loss : 104.87550354003906, val loss : 112.49714660644531\n",
            "Start of 28 epoch\n",
            "\t Train loss : 104.81361389160156, val loss : 112.53941345214844\n",
            "Start of 29 epoch\n",
            "\t Train loss : 104.1357650756836, val loss : 112.03577423095703\n",
            "Start of 30 epoch\n",
            "\t Train loss : 105.16358184814453, val loss : 111.96525573730469\n",
            "Start of 31 epoch\n",
            "\t Train loss : 104.79730987548828, val loss : 111.59062194824219\n",
            "Start of 32 epoch\n",
            "\t Train loss : 104.28960418701172, val loss : 111.61443328857422\n",
            "Start of 33 epoch\n",
            "\t Train loss : 104.26589965820312, val loss : 111.03355407714844\n",
            "Start of 34 epoch\n",
            "\t Train loss : 104.01789855957031, val loss : 110.48565673828125\n",
            "Start of 35 epoch\n",
            "\t Train loss : 104.12928771972656, val loss : 110.78776550292969\n",
            "Start of 36 epoch\n",
            "\t Train loss : 103.58241271972656, val loss : 111.23560333251953\n",
            "Start of 37 epoch\n",
            "\t Train loss : 103.72319793701172, val loss : 110.42814636230469\n",
            "Start of 38 epoch\n",
            "\t Train loss : 103.08831787109375, val loss : 110.22636413574219\n",
            "Start of 39 epoch\n",
            "\t Train loss : 103.60416412353516, val loss : 110.03160095214844\n",
            "Start of 40 epoch\n",
            "\t Train loss : 103.5835189819336, val loss : 109.75393676757812\n",
            "Start of 41 epoch\n",
            "\t Train loss : 102.57015991210938, val loss : 110.01738739013672\n",
            "Start of 42 epoch\n",
            "\t Train loss : 102.65437316894531, val loss : 109.69692993164062\n",
            "Start of 43 epoch\n",
            "\t Train loss : 102.67462158203125, val loss : 109.30690002441406\n",
            "Start of 44 epoch\n",
            "\t Train loss : 102.67527770996094, val loss : 109.86167907714844\n",
            "Start of 45 epoch\n",
            "\t Train loss : 102.43151092529297, val loss : 109.17454528808594\n",
            "Start of 46 epoch\n",
            "\t Train loss : 102.67622375488281, val loss : 109.18209838867188\n",
            "Start of 47 epoch\n",
            "\t Train loss : 102.41104888916016, val loss : 109.15878295898438\n",
            "Start of 48 epoch\n",
            "\t Train loss : 102.23495483398438, val loss : 108.95221710205078\n",
            "Start of 49 epoch\n",
            "\t Train loss : 101.97503662109375, val loss : 109.18798828125\n",
            "Start of 50 epoch\n",
            "\t Train loss : 102.33507537841797, val loss : 108.71565246582031\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJSuXBaiItel",
        "outputId": "5048f222-2a6a-4b7a-a68e-5d05821172fe"
      },
      "source": [
        "y_pred = model1(x_test)\n",
        "print(mse(y1_test,y_pred))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "109.05284380381967\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "AymIBWYEJonb",
        "outputId": "b8be5d75-01cf-4570-9d17-1955983765d3"
      },
      "source": [
        "plt.plot(list(range(epochs)),train_losses, label = 'Train losses')\n",
        "plt.plot(list(range(epochs)), val_losses, label = 'Validation losses')\n",
        "plt.legend()\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('Losses')\n",
        "plt.show()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qo8kvgm_M7bx"
      },
      "source": [
        "We can see that loss reduces significantly on training. However this model doesnot perform as good as our previous models. \n",
        "\n",
        "Training dataset was shuffled, entire procedure of compile and fit was implemented from scratch. Tuned the learning rate of optimizer and tried different optimizers and loss functions but did not gave results even close to regular model optimization and fitting. The loss was really very high in this case."
      ]
    }
  ]
}